# sql_query_agent.py

from langchain.schema import AIMessage
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_community.utilities import SQLDatabase
from langchain_community.tools.sql_database.tool import QuerySQLDatabaseTool
from langchain_community.chat_models import ChatOllama

from typing import Annotated

from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

from langsmith import traceable

from pathlib import Path
import os

# Import config for models and SQL agent settings
from config import (
    MODELS, 
    MODEL_TEMPERATURES,
    SQL_MAX_RETRY_LIMIT,
    SQL_MESSAGE_CONTEXT_LIMIT
)

### Load environment variables from .env file
from dotenv import load_dotenv

# Load the .env file from the project root directory (parent of the agents folder)
load_dotenv(dotenv_path=Path(__file__).parents[1] / '.env')


### Initialize the database

# path of current file
file_path = Path().resolve()
# Fix database path to use the correct location
database_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "database", "bt5153_gp.db")
db = SQLDatabase.from_uri('sqlite:///' + database_path)

# table schema information with example data
table_info = db.get_table_info()

# table schema information without example data
table_info_light = table_info.split('/*')
for i in range(len(table_info_light)):
    if '*/' in table_info_light[i]:
        table_info_light[i] = table_info_light[i].split('*/')[1]
table_info_light = '\n'.join(table_info_light)



### Initialize the LLM with standardized Ollama model config
llm = ChatOllama(model=MODELS["sql_agent"], temperature=MODEL_TEMPERATURES["sql_agent"])



### Initialize the safeguards
sql_errors_max_retry_limit = SQL_MAX_RETRY_LIMIT
message_context_limit = SQL_MESSAGE_CONTEXT_LIMIT



### Set up States
class State(TypedDict):
    '''
    Represents the application's state during execution. 
    Each key in this dictionary corresponds to a specific piece of data 
    used or updated during the workflow.
    '''
    # messages exchanged between the user and the LLM
    messages: Annotated[list, add_messages]

    # The question provided by the user, e.g., "What are the causes of readmissions?"
    question: str
    
    # The SQL query generated by the system to address the user's question.
    # Example: "SELECT cause, COUNT(*) FROM readmissions GROUP BY cause ORDER BY COUNT(*) DESC;"
    query: str
    
    # The result of executing the SQL query on the database.
    # Example: "Hypertension: 32, Diabetes: 28, Others: 15"
    result: str
    
    # The final answer synthesized by the system, combining the SQL query result
    # and the context of the user's question.
    # Example: "The most common cause of readmissions is Hypertension, with 32 occurrences."
    answer: str

    # The error messages from executing SQL code if there is an error
    error: Annotated[list, add_messages]



### Format user input
def format_input(user_input):       # not added to langgraph as a node
    """Format user input into a human message for the LLM."""
    print('User: ', user_input)
    return {'messages': HumanMessage('User: ' + user_input)}



### Set up agent
def classify_input(question):
    """Evaluate if the answer to the user input question can be found in the database."""
    prompt = (
        f"Given a user question and SQLite database schema information, determine if the question can be answered using the SQLite database.\n\n"
        f"User question: {question}\n\n"
        f"Database schema:\n{table_info}\n\n"
        f"Instructions:\n"
        f"1. Analyze the SQLite database schema to understand available tables, columns, and their relationships.\n"
        f"2. Identify primary keys (PK) and foreign keys (FK) between tables.\n"
        f"3. If the question requires data from multiple tables, determine the appropriate join conditions based on PK/FK relationships.\n"
        f"4. For multi-table queries, always specify the table name for columns (table.column) to avoid ambiguity.\n"
        f"5. You may use GROUP BY, ORDER BY, and LIMIT clauses to enhance the query results.\n"
        f"6. First, think step-by-step about how to answer this question using SQL.\n"
        f"7. Then, answer with either:\n"
        f"   - 'Yes' if the question can be answered\n"
        f"   - 'No' if the question cannot be answered\n"
        f" Do not include any other text in your response.\n\n"
    )
    response = llm.invoke(prompt)
    
    # Extract decision from response
    response_lower = response.content.lower()
    if response_lower.startswith('yes'):
        return True
    else:
        return False

@traceable(tags=["agent:sql_query_agent", "function:clarify_with_user"])
def clarify_with_user(state: State):
    """Explain to the user why their question cannot be answered using the database information and ask for clarifications."""
    user_input = state['messages']
    if len(user_input) > message_context_limit:
        user_input = user_input[-message_context_limit:]
    prompt = (
        f"Given a user question and schema of the SQL database, explain why the question may not be able to be answered.\n\n"
        f"User question: {user_input}\n\n"
        f"Information of the SQL database schema: {table_info_light}\n\n"
        f"Explain what required information may be needed to answer the user question that is currently missing."
        f"Clarify with the user what are they trying to ask and what information they need to answer their question."
    )
    response = llm.invoke(prompt)
    # print('[DEBUG] Clarification:', response.content)  # Print the raw answer
    return {'answer': response.content}

@traceable(tags=["agent:sql_query_agent", "function:format_question"])
def format_question(state: State):
    """Understand the context and breakdown the question the user is asking."""
    new_message = state['messages'][-1]
    message_context = state['messages'][:-1]
    # print('[DEBUG] Context:', message_context)
    prompt = (
        f"Given a chat context and a new user message, understand the context and breakdown the question the user is asking.\n\n"
        f"Chat context:\n{message_context}\n\n"
        f"User new message:\n{new_message}\n\n"
        f"Database information:\n{table_info_light}\n\n"
        f"Breakdown the question to extract the key information that the user is asking for in the data.\n\n"
        f"Only answer with the key information that the user would like to know, do not include any other text in your response.\n\n"
    )
    response = llm.invoke(prompt)
    # print('[DEBUG] Question Breakdown:', response.content)  # Print the raw answer
    classification = classify_input(response.content)
    if classification:
        question = response.content + '\nPlease parse the SQL returned for common words/patterns.'
        return {'route': 'write_query', 'question': question}
    else:
        return {'route': 'clarify_with_user'}


def preprocess_query(query):
    """Clean and preprocess SQL query for SQL Server."""

    # Remove Markdown-style SQL code blocks (```sql ... ```)
    if '```' in query:
        query = query.split('```')[1].split('```')[0].strip('\n').strip('sql')

    # Replace MySQL-style backticks (`) with SQL Server-style square brackets []
    if '```' in query:
        query = query.replace('`', '')

    # Split the query into lines and remove any comments (`--` style)
    query_lines = query.splitlines()
    sql_lines = [line for line in query_lines if not line.strip().startswith('--')]

    # Combine the cleaned lines back into a single SQL query
    query = ' '.join(sql_lines)

    # Remove trailing comments or explanations after the first semicolon
    if ';' in query:
        query = query.split(';')[0] + ';'  # Keep only the first valid SQL segment

    return query

@traceable(tags=["agent:sql_query_agent", "function:write_query"])
def write_query(state: State):
    """Generate SQL query from a user question using the LLM."""
    question = state['question']
    # print(f'[DEBUG] Question: {question}')  # Debug print
    error_context = ''
    if len(state['error']) > 0:
        error_context = "\n\nPrevious query attempts failed with the following errors:\n"
        error_context += f"{state['error']}\n"
    prompt = (
        f"Generate a SQLite query to answer the following question:\n\n"
        f"Question: {question}\n"
        f"Use the schema below:\n{table_info}\n\n"
        f"Please follow these instructions and think step by step:\n\n"
        f"1. Analyze the SQLite database schema to understand available tables, columns, and their relationships.\n"
        f"2. Identify primary keys (PK) and foreign keys (FK) between tables.\n"
        f"3. If the question requires data from multiple tables, determine the appropriate join conditions based on PK/FK relationships.\n"
        f"4. For multi-table queries, always specify the table name for columns (table.column) to avoid ambiguity.\n"
        f"5. If certain fields already exists as columns, then you do not need to calculate them, for instance there is no need to calculate arrival_date - discharge_date if length_of_stay already exists.\n"
        f"6. You may use JOIN, GROUP BY, ORDER BY, and LIMIT clauses to enhance the query results.\n"
        f"7. First, think step-by-step about how to answer this question using SQL.\n"
        f"8. Strictly return only the SQL query as a code block."
        f"{error_context}"
    )
    response = llm.invoke(prompt)
    # print('[DEBUG] Raw Query from LLM:', response.content)  # Print the raw query

    # Preprocess the query to clean up formatting
    query = preprocess_query(response.content)
    # print('[DEBUG] Preprocessed Query:', query)  # Print the preprocessed query
    return {'query': query}


@traceable(tags=["agent:sql_query_agent", "function:execute_query"])
def execute_query(state: State):
    """Execute SQL query."""
    try:
        print('[DEBUG] Attempting to execute query:', state['query'])  # Debug print
        execute_query_tool = QuerySQLDatabaseTool(db=db)
        result = execute_query_tool.invoke(state['query'])
        # print('[DEBUG] Raw Result from LLM:', result)  # Debug print
        if 'Error:' in result and len(state['error']) < sql_errors_max_retry_limit:
            return {'error': SystemMessage(content=result), 'route': 'write_query'}
        else:
            return {'result': result, 'route': 'generate_answer'}
    except Exception as e:
        error_msg = f'Query execution error: {str(e)}'
        print('[DEBUG] ', error_msg)  # Debug print
        return {'error': SystemMessage(content=error_msg), 'route': 'write_query'}

@traceable(tags=["agent:sql_query_agent", "function:generate_answer"])
def generate_answer(state: State):
    """Answer question using retrieved information as context."""
    prompt = (
        f"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n\n"
        f"Question: {state['question']}\n"
        f"SQL Query: {state['query']}\n"
        f"SQL Result: {state['result']}\n"
        f"Explain the answer clearly and professionally, making sense of the results and numbers.\n"
        f"Only return the answer without repeating the question or explaining the query.\n"
        f"Explicity state the source of the SQL data in the answer, including the table names and column names used.\n\n"
        f"Answer 'The question cannot be answered' if it cannot be answered."
    )
    response = llm.invoke(prompt)
    # print('[DEBUG] Raw Answer from LLM:', response.content)  # Print the raw answer
    return {'answer': response.content}

def format_response(state: State):
    '''Format answer into message for the user.'''
    return {'messages': AIMessage(state['answer'])}

def create_graph():
    memory = MemorySaver()
    graph_builder = StateGraph(State)
    graph_builder.add_node('format_question', format_question)
    graph_builder.add_node('clarify_with_user', clarify_with_user)
    graph_builder.add_node('write_query', write_query)
    graph_builder.add_node('execute_query', execute_query)
    graph_builder.add_node('generate_answer', generate_answer)
    graph_builder.add_node('format_response', format_response)

    graph_builder.add_edge(START, 'format_question')
    graph_builder.add_conditional_edges('format_question', lambda state: state['route'], {'write_query': 'write_query', 'clarify_with_user': 'clarify_with_user'})
    graph_builder.add_edge('write_query', 'execute_query')
    graph_builder.add_conditional_edges('execute_query', lambda state: state['route'], {'write_query': 'write_query', 'generate_answer': 'generate_answer'})
    graph_builder.add_edge('generate_answer', 'format_response')
    graph_builder.add_edge('clarify_with_user', 'format_response')
    graph_builder.add_edge('format_response', END)
    return graph_builder.compile(checkpointer=memory)