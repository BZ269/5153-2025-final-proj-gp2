# SQL Agent Evaluator

This tool evaluates the performance of the LangGraph SQL agent by processing a set of test queries and recording the results.

## Files

- `test_queries.csv`: Contains test queries to be processed by the SQL agent.
- `sql_agent_evaluator.py`: Python script that processes queries and records results.
- Results are stored in the main `evaluation/results/` directory.

## How to Use

1. Ensure your SQL agent and database are properly set up.
2. Customize `test_queries.csv` with the queries you want to test.
3. Run the evaluation script:

```bash
python sql_agent_evaluator.py
```

4. Results will be saved in the main `evaluation/results/` directory with a timestamp in the filename (format: `sql_evaluation_results_YYYYMMDD_HHMMSS.csv`).

## Output

The evaluation script generates two files for each run:

1. A CSV file with detailed results for each query:
   - `query_id`: The ID of the query
   - `query_text`: The original query text
   - `sql_query`: The SQL query generated by the agent
   - `query_result`: The raw result from executing the SQL query
   - `answer`: The natural language answer generated from the SQL results
   - `response_time_seconds`: The time taken to process the query
   - `error_messages`: Any error messages encountered

2. A summary text file with aggregate statistics:
   - Total number of queries processed
   - Total and average execution time
   - Path to the results file

## Creating Custom Test Sets

You can create your own test query sets by modifying the `test_queries.csv` file. The file should have a header row with 'test_query' as the column name, followed by one query per line.

Example:
```
test_query
What is the total number of patients in the database?
Which demographic has the highest readmission rate?
...
```

## Troubleshooting

- If the script fails while running, check the backup CSV file created in the same directory.
- Make sure the SQL agent and database are properly configured before running the evaluation.
- For large sets of queries, consider running the evaluation in batches to avoid potential timeout issues. 